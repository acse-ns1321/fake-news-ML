{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810c8c1b",
   "metadata": {},
   "source": [
    "# Backprop-From-Scratch using Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97cb82a",
   "metadata": {},
   "source": [
    "In the following exercise you will implement a simple 1-Hidden Layer Neural Network from scratch using torch. We will use a simple toy dataset called the half-moon dataset as a training set. To perform this task we will outline the necessary steps here and provide you with pseudo-code for your implementation.\n",
    "\n",
    "1. Set the random number generator\n",
    "\n",
    "2. Which hyper-parameters will you need to set prior to training?\n",
    "\n",
    "3. Define the size of the input layer (D), the number of hidden layer units (H) and the output layer units (M).\n",
    "    - Suggestion: use a small number of neurons in the hidden layer e.g. H=3\n",
    "    \n",
    "    \n",
    "4. Define the training and test sets of your half-moon dataset.\n",
    "\n",
    "5. We will use ```sigmoid``` activation functions. Define functions to compute the forward and \"backward\"-pass of the sigmoid. Your function should take in a ```torch.Tensor``` and return a ```torch.Tensor```\n",
    "\n",
    "\n",
    "6. Define the weight tensors of each-layer. Initiallize the Weight-Tensors as ```torch.randn```. You should have two weight tensors W1, W2.\n",
    "\n",
    "\n",
    "7. Within a training loop perform the following operations for the forward pass\n",
    "    - Compute the affine layer transformation $z_1=W_1X$\n",
    "    - Compute the non-linear activation $a_1=\\sigma(z_1)$\n",
    "    - Compute the affine layer transformation $z_2=W_2a_1$\n",
    "    - Compute the non-linear activation $a_2=\\sigma(z_2)$\n",
    "    - Recall the chain-rule:\n",
    "    - Use the notes at the bottom to simplify the code\\*\n",
    "    - Compute the gradient of the Loss with respect to the weights of the output layer $\\frac{\\partial{L}}{\\partial{W_2}}=a_1^T*\\frac{\\partial{L}}{\\partial{a_2}}\\frac{\\partial{a_2}}{\\partial{z_2}}$. You will need to use ```torch.transpose``` and ```torch.matmul``` to perform this operation.\n",
    "    - Compute the error on the output of the hidden-layer $\\frac{\\partial{L}}{\\partial{a_1}}$\n",
    "    - Compute the gradient of the loss with respect to the hidden-layer weights $W_1$. This is the same operation as for the output layer.\n",
    "    - Bonus: Compute the sensitivity of the loss with respect to the input $\\frac{\\partial{L}}{\\partial{X}}$\n",
    "    - Perform a gradient descent step on the weights: $W_2^{t+1} = W_2^{t}-\\frac{\\alpha}{N}\\frac{\\partial{L}}{\\partial{W_2}}$. (Hint: the division by $N$ is necessary due to the ```torch.matmul``` operation being an effective summation over all the input examples.\n",
    "    - Compute the training loss as the binary cross entropy $BCE(y, a_2)=\\frac{1}{N}\\sum{y\\cdot log(a_2)+(1-y)\\cdot log(1-a_2)}$\n",
    "    \n",
    "    \n",
    "    \n",
    "8. Perform the above iteration over a number of epochs (full-passes through the training set and use full-batch learning)\n",
    "\n",
    "\n",
    "\n",
    "8. After training, evaluate the performance on the test set by evaluating $y_{pred}=\\sigma(W_2\\sigma(W_1 X))$ and computing the accuracy using ```sklearn.metrics.accuracy_score```.\n",
    "\n",
    "\n",
    "\n",
    "9. Plot the prediction on the training and the test set.\n",
    "\n",
    "\n",
    "\\\n",
    "10. Bonus: Plot the sensitivity of the loss with respect to each datapoint in the input of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3188433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# STEP 1 : Set the seed \n",
    "set_seed(42)\n",
    "\n",
    "# STEP 2 :Set the Hyperparameters\n",
    "epochs = 1000 #Number of loops through whole dataset\n",
    "batch_size = 1000 #Size of a single batch\n",
    "batch_num = 1 #Use full batch training\n",
    "test_size = 100 #Examples in test set\n",
    "\n",
    "# STEP 3 : Set the parameters for the neural network - learning rate, network size\n",
    "lr = 1.\n",
    "I, H, O = 2, 500, 1 #Define input size (2), Size of Hidden Layer (4), Output size (1)\n",
    "\n",
    "\n",
    "# STEP 4 : Define training and test sets for two moon dataset\n",
    "X_train, y_train, X_test, y_test = make_train_test(batch_size, batch_num, test_size, noise=0.2)\n",
    "\n",
    "#Define Train Set in Pytorch\n",
    "X = torch.from_numpy(X_train).float()[0] #Convert to torch tensor, single batch\n",
    "y = torch.from_numpy(y_train).float()[0] #Convert to torch tensor, single batch\n",
    "\n",
    "#Define Test Set in Pytorch\n",
    "X_test = torch.from_numpy(X_test).float() #Convert to torch tensor, already single batch\n",
    "y_test = torch.from_numpy(y_test).float() #Convert to torch tensor, already single batch\n",
    "\n",
    "\n",
    "# STEP 5 : Activation Function and derivative of the activation function\n",
    "sigmoid = lambda x: 1./(1+torch.exp(-x)) #Sigmoid Activation Function\n",
    "dSigmoid = lambda x: x*(1-x) #Derivative of Sigmoid Activation Function\n",
    "\n",
    "# STEP 6 : Define the weight tensors\n",
    "W1 = torch.randn((I, H))\n",
    "W2 = torch.randn((H,O))\n",
    "\n",
    "\n",
    "# STEP 7 : Forward Pass\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # store number of inputs\n",
    "    N n= X.size(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
